```markdown
# Creating a Genomics Nextflow Pipeline on a University HPC Cluster

This guide provides a step-by-step introduction to creating and running a simple genomics pipeline using Nextflow on a university HPC cluster. It will cover basic Nextflow syntax, HPC cluster concepts, and containerization for reproducibility, using a `FastQC` example.

**Target Audience:** Researchers and students with basic command-line proficiency and an understanding of genomics, but no prior experience with Nextflow or HPC cluster environments.

## 1. Introduction

### 1.1. The Need for Genomics Pipelines

Genomics research generates vast amounts of data, from DNA sequencing reads to gene expression measurements. Analyzing this data requires complex and often repetitive computational workflows, or pipelines, which involve multiple steps like quality control, alignment, variant calling, and annotation. Manually managing these steps can be time-consuming, error-prone, and difficult to reproduce. Genomics pipelines automate these processes, ensuring efficient, reliable, and reproducible results. Without pipelines, researchers would struggle to efficiently extract meaningful insights from genomic data.

**Example:** Imagine sequencing the genomes of 100 different bacterial strains to identify antibiotic resistance genes. Manually analyzing each strain's sequencing data, performing quality control, aligning the reads to a reference genome, and then searching for resistance genes would be incredibly laborious and prone to human error. A genomics pipeline automates all of these steps, allowing the researcher to focus on interpreting the results rather than managing the computational process.

**Visual Aids:** A 'before and after' illustration showing a researcher struggling with manual data processing vs. a researcher efficiently using a genomics pipeline.

**Supporting Materials:** Link to a paper or article highlighting the challenges of manual bioinformatics data analysis.

### 1.2. Introducing Nextflow and HPC Clusters

Nextflow provides a powerful and flexible framework for building genomics pipelines. It simplifies the creation of complex workflows by automatically managing data flow, parallel execution, and software dependencies. Nextflow's domain-specific language (DSL) makes it easy to define pipelines in a clear and concise manner. HPC clusters offer the necessary computational power to handle the large datasets generated by genomics research. By combining Nextflow with an HPC cluster, researchers can efficiently process and analyze genomic data at scale. Nextflow can leverage the scheduler on the HPC cluster to distribute tasks across multiple nodes, significantly reducing processing time.

**Example:** A pipeline for analyzing RNA sequencing data might involve aligning millions of reads to a reference genome. This task can be highly computationally intensive, especially for large genomes. By running the alignment step on an HPC cluster using Nextflow, the reads can be processed in parallel across multiple nodes, significantly reducing the total processing time.

**Visual Aids:** An animation illustrating how Nextflow distributes tasks across multiple nodes in an HPC cluster for parallel processing.

**Supporting Materials:** Link to a success story or case study of using Nextflow on an HPC cluster for genomics research.

### 1.3. Purpose and Scope of this Guide

This guide will provide a step-by-step introduction to creating and running a simple genomics pipeline using Nextflow on a university HPC cluster. You will learn how to define a pipeline, configure Nextflow to use the cluster's resources, and submit your pipeline for execution. We will cover basic Nextflow syntax, HPC cluster concepts, and containerization for reproducibility. This guide focuses on practical examples to get you started quickly. It does *not* cover advanced Nextflow features such as complex data structures, custom operators, or cloud deployment. It also assumes you have basic familiarity with the command line and genomics concepts. We also assume your university HPC cluster provides some level of support for containers.  We will use a `FastQC` example. This guide is designed to provide a solid foundation for building more complex genomics pipelines in the future.

**Visual Aids:** A roadmap or outline showing the topics covered in the guide.

**Supporting Materials:** Link to a list of other Nextflow tutorials or resources for further learning.

## 2. Understanding the Basics

### 2.1. Key Concepts Revisited

Before diving into the details, let's quickly revisit the key concepts that underpin this guide. Please refer back to the 'Key Concepts' section below for detailed definitions of Genomics, Nextflow, Pipelines, HPC Clusters, Schedulers, Containers, Modules, DSL, Reproducibility and Portability. These concepts are fundamental to understanding how to build and run genomics pipelines efficiently and reproducibly.

**Visual Aids:** A mind map or concept map connecting all the key concepts together.

**Supporting Materials:** A glossary of bioinformatics and HPC terms.

### 2.2. Introduction to Nextflow Syntax

#### 2.2.1. Channels

Channels are the fundamental building blocks for data flow in Nextflow. They act as queues, allowing processes to communicate and pass data between each other. Think of them as pipes through which data flows from one step in your pipeline to the next. Channels can be defined in various ways, such as from a list of files, a directory, or even from the output of another process.

**Example:**

```nextflow
// Create a channel from a list of fastq files
Channel.fromPath('data/*.fastq.gz')
       .set { reads_channel }
```

In this example, `Channel.fromPath('data/*.fastq.gz')` creates a channel that emits the path to each fastq file found in the `data` directory. The `.set { reads_channel }` assigns this channel to the variable `reads_channel` so that it can be used as input to a process.

**Visual Aids:** A diagram illustrating data flowing through a channel from one process to another.

**Supporting Materials:** Link to the Nextflow documentation section on channels: [https://www.nextflow.io/docs/latest/channel.html](https://www.nextflow.io/docs/latest/channel.html)

#### 2.2.2. Processes

Processes are the individual tasks or steps in your Nextflow pipeline. They define the command or script that will be executed, along with the input data they require and the output data they produce. Processes are defined using the `process` keyword, followed by a name and a block of code that specifies the execution details. Crucially, processes define the *smallest* unit of parallel execution. Each process invocation is independent.

**Example:**

```nextflow
process fastqc {
  input:
    file reads from reads_channel

  output:
    file 'fastqc_report.html' into fastqc_reports

  container 'quay.io/biocontainers/fastqc:0.11.9--0'

  script:
    """
    fastqc ${reads} -o .
    """
}
```

In this example, the `fastqc` process takes a `reads` file from the `reads_channel` as input. It then executes the `fastqc` command to generate a quality control report. The `output` declaration specifies that the file named `fastqc_report.html` is the output of this process and sends it to the `fastqc_reports` channel. The `container` directive specifies which container image the process should run within. The script directive defines the shell commands to run inside the container.

**Visual Aids:** A diagram illustrating the structure of a Nextflow process, highlighting input, output, container, and script directives.

**Supporting Materials:** Link to the Nextflow documentation section on processes: [https://www.nextflow.io/docs/latest/process.html](https://www.nextflow.io/docs/latest/process.html)

#### 2.2.3. Workflows

Workflows define the overall structure of your pipeline by connecting processes together. They specify the order in which processes are executed and how data is passed between them. Workflows are defined using the `workflow` keyword, followed by a name and a block of code that defines the data flow.

**Example:**

```nextflow
workflow {
  fastqc(reads_channel)
}
```

In this example, the workflow simply calls the `fastqc` process, passing the `reads_channel` as input. This tells Nextflow to execute the `fastqc` process for each item in the `reads_channel`.

More complex workflows can involve multiple processes chained together. For example:

```nextflow
workflow {
  Channel.fromPath('data/*.fastq.gz').set { reads_channel }
  fastqc(reads_channel)
  // another_process(fastqc.out)
}
```

In this example, the workflow first defines the `reads_channel` from fastq files in the data directory. Then, the data in that channel is used as input into the `fastqc` process. A commented out line shows how the output of the `fastqc` process *could* be fed into another process named `another_process`.

**Visual Aids:** A flowchart illustrating a simple workflow with two processes chained together.

**Supporting Materials:** Link to the Nextflow documentation section on workflows: [https://www.nextflow.io/docs/latest/workflow.html](https://www.nextflow.io/docs/latest/workflow.html)

#### 2.2.4. Parameters

Parameters allow you to define configurable values that can be passed to your Nextflow pipeline at runtime. This makes your pipeline more flexible and reusable. Parameters can be defined in the `nextflow.config` file or passed directly via the command line using the `-params` option or using command line arguments.

**Example:**

In `nextflow.config`:

```nextflow
publishDir = 'results'
referenceGenome = '/path/to/genome.fasta'
```

In `main.nf`:

```nextflow
process align {
  input:
    file reads from reads_channel

  output:
    file 'aligned.bam' into aligned_bams

  script:
    """
    bwa mem -t 8 ${params.referenceGenome} ${reads} > aligned.bam
    """
}

workflow {
   Channel.fromPath(params.reads).set { reads_channel }
   align(reads_channel)
}

params.reads = 'data/*.fastq.gz'
```

In this example, `publishDir` and `referenceGenome` are defined as parameters in the `nextflow.config` file. The `align` process uses the `referenceGenome` parameter to specify the path to the reference genome. The `reads` parameter is defined in the `main.nf` and can be overwritten at runtime. To run this pipeline, you could execute: `nextflow run main.nf -params.referenceGenome /alternative/path/to/genome.fasta -params.reads 'other_data/*.fastq.gz'` This will override the default values defined in the configuration files and the script.

**Visual Aids:** A diagram showing how parameters are defined in `nextflow.config` and `main.nf` and how they affect the execution of a pipeline.

**Supporting Materials:** Link to the Nextflow documentation section on parameters: [https://www.nextflow.io/docs/latest/config.html#params](https://www.nextflow.io/docs/latest/config.html#params)

### 2.3. Introduction to HPC Cluster Architecture

#### 2.3.1. Nodes and Resources (CPU, Memory)

An HPC cluster consists of multiple interconnected computers called *nodes*. Each node is a self-contained computing unit with its own CPU, memory (RAM), and storage. The CPU (Central Processing Unit) is the brain of the computer, responsible for executing instructions. Memory (RAM) is used to store data and instructions that the CPU is actively working with. The amount of CPU cores and memory available on each node varies depending on the cluster's configuration. When you submit a job to the cluster, you request a certain number of CPU cores and a certain amount of memory to be allocated to your job.

**Example:** A typical HPC cluster node might have 24 CPU cores and 128 GB of RAM. If your pipeline requires a lot of processing power, you might request multiple nodes or a large number of cores on a single node. If your pipeline requires a large amount of memory, you need to ensure that you request enough memory to avoid running out of memory and causing your job to fail.

**Visual Aids:** An illustration of a single HPC node with labeled components (CPU, RAM, storage).

**Supporting Materials:** Link to a resource explaining the basics of CPU and RAM.

#### 2.3.2. Job Schedulers (SLURM, PBS)

Job schedulers are software systems that manage and schedule jobs on an HPC cluster. They allocate resources (CPU cores, memory, etc.) to jobs and ensure that they are executed efficiently. Common job schedulers include SLURM (Simple Linux Utility for Resource Management) and PBS (Portable Batch System). You interact with the job scheduler to submit your Nextflow pipeline as a job to the cluster. The scheduler then places your job in a queue and executes it when the requested resources become available.

**Example:**

*   **SLURM:**
    *   `sbatch <script.sh>`: Submits a job defined in the `script.sh` file.
    *   `squeue`: Shows the status of jobs in the queue.
    *   `scancel <job_id>`: Cancels a job with the specified `job_id`.
    *   `sinfo`: Displays information about the cluster's nodes and resources.
*   **PBS:**
    *   `qsub <script.sh>`: Submits a job defined in the `script.sh` file.
    *   `qstat`: Shows the status of jobs in the queue.
    *   `qdel <job_id>`: Cancels a job with the specified `job_id`.
    *   `pbsnodes`: Displays information about the cluster's nodes and resources.

These commands allow you to interact with the scheduler to manage your jobs on the cluster.

**Visual Aids:** A diagram showing the job queue in a scheduler and how jobs are allocated resources based on priority and availability.

**Supporting Materials:** A table comparing common SLURM and PBS commands and their functions.

#### 2.3.3. File Systems (Shared vs. Local)

HPC clusters typically have two types of file systems: shared file systems and local file systems. The *shared file system* is accessible from all nodes in the cluster. This is where you typically store your input data, Nextflow scripts, and configuration files. The *local file system* is specific to each node and is usually faster than the shared file system. However, data stored on the local file system is not accessible from other nodes. It also might get deleted once the job finishes. For this reason, it's generally preferable to read/write to the shared file system. Using the local file system can be advantageous for very large intermediate files that don't need to be shared between processes, but you'll need to explicitly copy files to and from the local disk in your Nextflow script. Also, the local disk might not be available and/or large enough for your files.

**Example:** Your input FASTQ files and Nextflow script (`main.nf`) would typically be stored on the shared file system (e.g., `/home/<username>/project/`). When a Nextflow process runs on a specific node, it can access these files directly from the shared file system. If a process generates intermediate files that are only needed by subsequent processes running on the same node, you *might* consider using the local file system for those files to improve performance, but remember to handle file transfers explicitly in the pipeline definition.

**Visual Aids:** A diagram illustrating the difference between shared and local file systems on an HPC cluster, showing which nodes have access to each.

**Supporting Materials:** A flowchart to help users decide whether to use the shared or local file system for their data.

## 3. Setting Up Your Environment on the HPC Cluster

### 3.1. Accessing the HPC Cluster

Accessing an HPC cluster typically involves using SSH (Secure Shell), a secure protocol that allows you to connect to a remote computer over a network. You'll need an SSH client (e.g., PuTTY on Windows, Terminal on macOS/Linux) and your cluster login credentials (username and password).

**Example:**

1.  Open your terminal or SSH client.
2.  Type the following command, replacing `<username>` with your actual username and `<cluster_address>` with the cluster's address:

    ```bash
    ssh <username>@<cluster_address>
    ```

    For example:

    ```bash
    ssh jdoe@hpc.university.edu
    ```
3.  You'll be prompted for your password. Enter it carefully (it won't be displayed on the screen).  If you have set up SSH keys, you won't need to enter a password.
4.  Once authenticated, you'll be logged into the cluster's command-line interface.

**Visual Aids:** Screenshots of popular SSH clients (PuTTY, Terminal) showing the connection process.

**Supporting Materials:** Link to a tutorial on how to set up SSH keys for passwordless login to an HPC cluster.

### 3.2. Installing Nextflow (if needed)

Most university HPC clusters will have Nextflow pre-installed as a module. However, if it's not available or you need a specific version, you can install it in your home directory. This requires Java to be installed. Consult your HPC cluster documentation to find out which version of Java is installed by default or use a module. Then, you can download the Nextflow installation script and run it.

**Example:**

1.  Download the Nextflow installation script:

    ```bash
    curl -fsSL get.nextflow.io | bash
    ```
2.  This will download the `nextflow` executable to your current directory. You can move it to a directory in your `PATH` (e.g., `~/bin`) to make it accessible from anywhere. First, create the bin folder if it doesn't exist:

    ```bash
    mkdir -p ~/bin
    ```
3.  Move the Nextflow executable:

    ```bash
    mv nextflow ~/bin/
    ```
4.  Add `~/bin` to your `PATH` environment variable. Edit your `~/.bashrc` or `~/.zshrc` file and add the following line:

    ```bash
    export PATH=$PATH:~/bin
    ```
5.  Source your `.bashrc` or `.zshrc` file to apply the changes:

    ```bash
    source ~/.bashrc
    ```

    or

    ```bash
    source ~/.zshrc
    ```
6.  Verify that Nextflow is installed correctly by running:

    ```bash
    nextflow -v
    ```

    This should print the Nextflow version number.

**Visual Aids:** A screencast demonstrating the steps to install Nextflow on an HPC cluster.

**Supporting Materials:** Link to the Nextflow installation guide: [https://www.nextflow.io/docs/latest/getstarted.html#installation](https://www.nextflow.io/docs/latest/getstarted.html#installation)

### 3.3. Configuring Nextflow for the Cluster

#### 3.3.1. Editing `nextflow.config`

The `nextflow.config` file is where you configure Nextflow to use the cluster's resources and job scheduler. This file specifies how Nextflow should execute processes, including which executor to use, how many resources to request, and where to store temporary files.  This file can be in your working directory (where you run `nextflow run`) or in your home directory for global configuration.

**Example:** A basic `nextflow.config` file for SLURM might look like this:

```nextflow
process {
    executor = 'slurm'
    queue = 'your_queue_name' // Replace with the appropriate queue
}

executor {
    slurm {
        queue = 'your_queue_name'
        submitRateLimit = '1/1min' //Adjust this to avoid flooding the scheduler
    }
}

params {
    reads = 'data/*.fastq.gz'
}

// Configure default execution options
 wave.executor.name = 'slurm'
```

In this example, the `process.executor` directive tells Nextflow to use the `slurm` executor by default. The `executor.slurm.queue` directive specifies the SLURM queue to submit jobs to. You'll need to replace `'your_queue_name'` with the appropriate queue name for your cluster. Contact your system admin for the correct queue to use. `submitRateLimit` can be used to avoid flooding the SLURM scheduler. The `params.reads` directive sets the default value for the reads input parameter.

**Visual Aids:** Syntax highlighting for the `nextflow.config` file, visually separating different directives and parameters.

**Supporting Materials:** Link to the Nextflow configuration documentation: [https://www.nextflow.io/docs/latest/config.html](https://www.nextflow.io/docs/latest/config.html)

#### 3.3.2. Defining Executors (e.g., SLURM executor)

Executors are responsible for submitting and managing jobs on the HPC cluster. Nextflow supports various executors, including SLURM, PBS, LSF, and others. You need to define the appropriate executor in your `nextflow.config` file based on the job scheduler used by your cluster.  The `executor` block in the `nextflow.config` defines the configuration for a specific executor.  You can define multiple executors and specify which executor to use for each process.

**Example:**

```nextflow
executor {
    slurm {
        queue = 'your_queue_name'
        cpus = 1
        memory = '4 GB'
        time = '1h'
        submitRateLimit = '1/1min'
    }
    local {
       enabled = false  //disable local executor on HPC
    }
}

process {
    executor = 'slurm' // all processes use slurm by default.
}
```

In this example, we define the `slurm` executor and the `local` executor. We configure the `slurm` executor to use a specific queue, request 1 CPU, 4 GB of memory, and a maximum runtime of 1 hour. We then set all `process` directives to use the `slurm` executor as a default. Note that the `cpus`, `memory`, and `time` parameters defined here are default values; they can be overridden for each specific process in the Nextflow script (see next section).  It is generally a good idea to disable the `local` executor on an HPC cluster.

**Visual Aids:** A table comparing the configuration options for different Nextflow executors (SLURM, PBS, LSF).

**Supporting Materials:** Link to the Nextflow executor documentation: [https://www.nextflow.io/docs/latest/executor.html](https://www.nextflow.io/docs/latest/executor.html)

#### 3.3.3. Resource Requests (CPU, Memory, Time)

You can specify the resource requirements (CPU cores, memory, and maximum runtime) for each process in your Nextflow script or in the `nextflow.config` file. This allows you to optimize resource utilization and ensure that your jobs are executed efficiently.  Over-requesting resources will cause the job to wait longer in the queue. Under-requesting resources might cause the job to crash.

**Example:**

```nextflow
process fastqc {
  input:
    file reads from reads_channel

  output:
    file 'fastqc_report.html' into fastqc_reports

  container 'quay.io/biocontainers/fastqc:0.11.9--0'

  cpus = 2
  memory = '8 GB'
  time = '30min'

  script:
    """
    fastqc ${reads} -o .
    """
}
```

In this example, we specify that the `fastqc` process requires 2 CPU cores, 8 GB of memory, and a maximum runtime of 30 minutes.  These values will override any default values defined in the `nextflow.config` file for this specific process. Nextflow translates these values to scheduler options. For SLURM, this translates to `#SBATCH --cpus-per-task=2`, `#SBATCH --mem=8G`, and `#SBATCH --time=30`.  The exact syntax may vary depending on the scheduler. Be careful with the units; memory is often in MB or GB and time is in minutes, hours, or days.

**Visual Aids:** A screenshot showing how resource requests in Nextflow translate to corresponding options in a SLURM or PBS job submission script.

**Supporting Materials:** Link to documentation or examples for resource request best practices on the target HPC cluster.

### 3.4. Using Modules (if applicable)

Many HPC clusters use environment modules to manage software installations. Modules allow you to easily switch between different versions of software packages without causing conflicts.  You can load modules using the `module load` command. It's crucial to load the necessary modules before running your Nextflow pipeline to ensure that the required software is available in the environment.

**Example:**

1.  List available modules:

    ```bash
    module avail
    ```
2.  Load the required modules:

    ```bash
    module load java/1.8
    module load nextflow/22.04
    ```

    Replace `java/1.8` and `nextflow/22.04` with the appropriate module names and versions for your cluster. Check with your system administrator if you are unsure of what modules to load.

   You can include module loading commands directly in your Nextflow script by using the `beforeScript` directive in a process.  However, it is generally better practice to load modules in your job submission script.

   ```nextflow
   process example {
      beforeScript 'module load java/1.8; module load nextflow/22.04'
      ...
   }
   ```

**Visual Aids:** Screenshots of the `module avail`, `module load`, and `module list` commands, showing how to manage modules.

**Supporting Materials:** Link to the documentation for the Environment Modules system: [http://modules.sourceforge.net/](http://modules.sourceforge.net/)

## 4. Building a Simple Genomics Pipeline with Nextflow

### 4.1. Example Use Case (e.g., FastQC for quality control)

For this guide, we'll use a simple but common genomics task: quality control of raw sequencing reads using FastQC. FastQC is a tool that generates a report summarizing the quality of sequencing data, including per-base quality scores, adapter content, and other metrics. This information is essential for identifying potential problems with the sequencing data before proceeding with downstream analysis. We will build a Nextflow pipeline that takes a set of FASTQ files as input and runs FastQC on each file to generate a quality report. We will assume the FASTQ files are gzipped.

**Visual Aids:** A screenshot of a FastQC report, highlighting key metrics.

**Supporting Materials:** Link to the FastQC website: [https://www.bioinformatics.babraham.ac.uk/projects/fastqc/](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)

### 4.2. Creating the Nextflow Script (`main.nf`)

#### 4.2.1. Input Channels

The first step is to define the input channel, which will provide the list of FASTQ files to be processed. We'll use `Channel.fromPath` to create a channel from a glob pattern that matches all FASTQ files in a directory.

**Example:**

```nextflow
params.reads = 'data/*.fastq.gz'

Channel.fromPath(params.reads)
    .set { reads_channel }
```

This code creates a channel named `reads_channel` that emits the paths to all FASTQ files ending in `.fastq.gz` found in the `data` directory. The `params.reads` allows this location to be configurable.

**Directory Structure:**

Before running this example, create a directory named `data` and place some example FASTQ files in it. You can download example FASTQ files from public repositories or create dummy files for testing purposes. For example:

```bash
mkdir data
touch data/sample1.fastq.gz data/sample2.fastq.gz
```

**Visual Aids:** A directory tree diagram showing the `data` directory and example FASTQ files.

**Supporting Materials:** Link to a public repository of example FASTQ files (e.g., from the Sequence Read Archive (SRA)).

#### 4.2.2. The FastQC Process

Now, let's define the `fastqc` process, which will execute the FastQC tool on each FASTQ file. We'll specify the input channel, output file, and the command to run. We will use a containerized version of FastQC for reproducibility.

**Example:**

```nextflow
process fastqc {
  input:
    file reads from reads_channel

  output:
    file "${reads.base}_fastqc.html" into fastqc_reports
    file "${reads.base}_fastqc.zip"  into fastqc_reports

  container 'quay.io/biocontainers/fastqc:0.11.9--0'

  cpus = 2
  memory = '4 GB'
  time = '1h'

  script:
    """
    fastqc ${reads}
    """
}
```

In this example:

*   The `input` directive specifies that the process takes a file named `reads` from the `reads_channel`.
*   The `output` directive specifies that the process produces two files: a HTML report and a ZIP archive. The file names are dynamically generated using the `reads.baseName` variable, which represents the filename of the input file without the extension. These files are then sent to the `fastqc_reports` channel.
*   The `container` directive specifies that the process should be executed inside the `quay.io/biocontainers/fastqc:0.11.9--0` container image. This ensures that the process has access to all the necessary dependencies and that the results are reproducible.
*   The `cpus`, `memory`, and `time` directives specify the resource requirements for the process.
*   The `script` directive defines the shell command to be executed, which simply runs FastQC on the input file.

#### 4.2.3. The Workflow Definition

Finally, let's define the workflow that connects the input channel and the `fastqc` process.

**Example:**

```nextflow
workflow {
  Channel.fromPath(params.reads).set { reads_channel }
  fastqc(reads_channel)
}
```

This workflow first creates the `reads_channel` from the files specified by the `params.reads` parameter. It then calls the `fastqc` process, passing the `reads_channel` as input. This will execute the `fastqc` process for each file in the `reads_channel`.

The complete `main.nf` file will look like this:

```nextflow
params.reads = 'data/*.fastq.gz'

Channel.fromPath(params.reads)
    .set { reads_channel }

process fastqc {
  input:
    file reads from reads_channel

  output:
    file "${reads.base}_fastqc.html" into fastqc_reports
    file "${reads.base}_fastqc.zip"  into fastqc_reports

  container 'quay.io/biocontainers/fastqc:0.11.9--0'

  cpus = 2
  memory = '4 GB'
  time = '1h'

  script:
    """
    fastqc ${reads}
    """
}

workflow {
  Channel.fromPath(params.reads).set { reads_channel }
  fastqc(reads_channel)
}
```

**Visual Aids:** Syntax highlighting for the complete `main.nf` file.

**Supporting Materials:** A link to download the complete `main.nf` file.

### 4.3. Creating the Configuration File (`nextflow.config`)

Create a `nextflow.config` file to configure Nextflow for your HPC cluster.  The exact contents of this file will depend on your cluster's configuration.  Here is an example `nextflow.config` file for a SLURM-based cluster:

```nextflow
process {
    executor = 'slurm'
    queue = 'your_queue_name' // Replace with the appropriate queue
}

executor {
    slurm {
        queue = 'your_queue_name'
        submitRateLimit = '1/1min' //Adjust this to avoid flooding the scheduler
    }
}

params {
    reads = 'data/*.fastq.gz'
}

// Configure default execution options
 wave.executor.name = 'slurm'
```

**Important:** Replace `your_queue_name` with the name of the queue you are allowed to submit jobs to. Contact your HPC cluster administrators if you are unsure.

### 4.4. Running the Pipeline

1.  **Save your `main.nf` and `nextflow.config` files** in a directory on the shared file system of the HPC cluster.  Also place the example FASTQ files in the `data` directory.
2.  **Load the necessary modules:**

    ```bash
    module load nextflow
    module load java
    ```

    or whatever modules are required for your cluster.
3.  **Submit the pipeline for execution:**

    ```bash
    nextflow run main.nf
    ```

    Nextflow will parse the `main.nf` script and the `nextflow.config` file, submit the `fastqc` processes to the SLURM scheduler, and monitor their execution.
4.  **Monitor the progress of the pipeline** using the `squeue` command or the HPC cluster's web interface.
5.  **Check the output:** Once the pipeline has finished executing, the FastQC reports will be generated in the `results` directory (or the directory specified by the `publishDir` parameter in your `nextflow.config` file). Each FASTQ file will have a corresponding HTML report and ZIP archive.

**Visual Aids:** Screenshots of the command-line interface showing the pipeline execution and the `squeue` output.

**Supporting Materials:** Link to the Nextflow command-line interface documentation: [https://www.nextflow.io/docs/latest/cli.html](https://www.nextflow.io/docs/latest/cli.html)

## 5. Key Concepts

### Genomics
**Definition:** The study of genomes, which are the complete sets of DNA (including all of its genes) in an organism. Genomics research often involves analyzing large datasets of DNA sequences to understand biological processes, diseases, and evolution.
**Importance:** Understanding the scope of genomics helps contextualize the need for efficient data processing pipelines.
**Visual Aids:** An infographic illustrating the flow of information from DNA to RNA to protein and how genomics studies the entire genome.
**Supporting Materials:** Link to the National Human Genome Research Institute (NHGRI) website for more information about genomics: [https://www.genome.gov/](https://www.genome.gov/)

### Nextflow
**Definition:** A domain-specific language (DSL) built on Groovy that allows you to write portable and reproducible workflows. It simplifies the creation of complex, data-intensive pipelines by managing data flow, parallel execution, and dependency management.
**Importance:** Nextflow is the core technology for defining and running the genomics pipeline.
**Visual Aids:** A diagram illustrating the key components of Nextflow (channels, processes, workflows) and how they interact.
**Supporting Materials:** Link to the official Nextflow documentation: [https://www.nextflow.io/docs/](https://www.nextflow.io/docs/)

### Pipeline (Bioinformatics/Genomics)
**Definition:** A series of computational steps chained together to process raw biological data into meaningful results. Each step in the pipeline typically performs a specific task, such as quality control, alignment, variant calling, or annotation.
**Importance:** Understanding the pipeline concept is crucial for understanding the purpose of Nextflow in this context.
**Visual Aids:** A flowchart depicting a typical genomics pipeline (e.g., raw reads -> quality control -> alignment -> variant calling -> annotation).
**Supporting Materials:** Link to a review article on common bioinformatics pipelines for genomics data analysis.

### HPC (High-Performance Computing) Cluster
**Definition:** A collection of interconnected computers (nodes) that work together as a single system to provide high computational power. HPC clusters are used for tasks that require significant processing resources, such as analyzing large genomic datasets.
**Importance:** University HPC clusters provide the necessary computing power for running computationally intensive genomics pipelines.
**Visual Aids:** A diagram of a typical HPC cluster architecture, showing the head node, compute nodes, and network interconnect.
**Supporting Materials:** Link to a general resource about HPC clusters, such as a university's HPC center webpage or a Wikipedia article.

### Scheduler (e.g., SLURM, PBS)
**Definition:** Software that manages and schedules jobs on an HPC cluster. It allocates resources (CPU cores, memory